import os
import json
import base64
import hashlib
import time
import threading
import requests
from datetime import datetime
from typing import Any, Dict, Optional, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("GitHubStorage")


class GitHubStorage:
    """
    Ù†Ø¸Ø§Ù… ØªØ®Ø²ÙŠÙ† Ø¯Ø§Ø¦Ù… Ø¹Ù„Ù‰ GitHub - ÙŠØ­ÙØ¸ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø±ÙŠØ¨Ùˆ
    Ù…Ø¬Ø§Ù†ÙŠ - Ø¨Ù„Ø§ Ø­Ø¯ÙˆØ¯ Ø¹Ù…Ù„ÙŠØ© - ÙŠØ¨Ù‚Ù‰ Ù…Ø¯Ù‰ Ø§Ù„Ø­ÙŠØ§Ø©
    """
    
    def __init__(self):
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GitHub
        self.github_token = os.environ.get("GITHUB_TOKEN")
        self.repo_owner = os.environ.get("GITHUB_REPO_OWNER")  # Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        self.repo_name = os.environ.get("GITHUB_REPO_NAME")    # Ø§Ø³Ù… Ø§Ù„Ø±ÙŠØ¨Ùˆ
        self.branch = os.environ.get("GITHUB_BRANCH", "main")
        
        if not all([self.github_token, self.repo_owner, self.repo_name]):
            raise ValueError(
                "ÙŠØ¬Ø¨ ØªØ¹ÙŠÙŠÙ† GITHUB_TOKEN, GITHUB_REPO_OWNER, GITHUB_REPO_NAME"
            )
        
        self.base_url = f"https://api.github.com/repos/{self.repo_owner}/{self.repo_name}"
        self.headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json",
            "Content-Type": "application/json"
        }
        
        # ÙƒØ§Ø´ Ù…Ø­Ù„ÙŠ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª
        self._cache: Dict[str, Any] = {}
        self._cache_sha: Dict[str, str] = {}  # SHA Ù„ÙƒÙ„ Ù…Ù„Ù
        self._dirty: set = set()  # Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ø­ÙØ¸
        
        # Ù‚ÙÙ„ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ²Ø§Ù…Ù†
        self._lock = threading.Lock()
        
        # ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡
        self._load_all_data()
        
        # Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©
        self._auto_save_interval = 30
        self._start_auto_save()
        
        logger.info("âœ… Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¹Ù„Ù‰ GitHub Ø¬Ø§Ù‡Ø²!")
    
    # ============================================
    # Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ GitHub API
    # ============================================
    
    def _github_request(self, method: str, endpoint: str, 
                         data: dict = None, retries: int = 3) -> Optional[dict]:
        """Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ù„Ù€ GitHub API Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©"""
        url = f"{self.base_url}/{endpoint}"
        
        for attempt in range(retries):
            try:
                if method == "GET":
                    response = requests.get(url, headers=self.headers, timeout=30)
                elif method == "PUT":
                    response = requests.put(
                        url, headers=self.headers, 
                        json=data, timeout=30
                    )
                elif method == "DELETE":
                    response = requests.delete(
                        url, headers=self.headers, 
                        json=data, timeout=30
                    )
                
                if response.status_code in [200, 201]:
                    return response.json()
                elif response.status_code == 404:
                    return None
                elif response.status_code == 409:
                    # conflict - Ù†Ø­ØªØ§Ø¬ Ù†Ø­Ø¯Ø« SHA
                    logger.warning(f"âš ï¸ ØªØ¹Ø§Ø±Ø¶ ÙÙŠ {endpoint}, Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...")
                    time.sleep(1)
                    # ØªØ­Ø¯ÙŠØ« SHA
                    file_path = endpoint.replace("contents/", "")
                    self._refresh_sha(file_path)
                    continue
                elif response.status_code == 422:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ 422 ÙÙŠ {endpoint}, Ø¬Ø§Ø±ÙŠ ØªØ­Ø¯ÙŠØ« SHA...")
                    file_path = endpoint.replace("contents/", "")
                    self._refresh_sha(file_path)
                    continue
                elif response.status_code == 403:
                    # Rate limit
                    reset_time = int(
                        response.headers.get("X-RateLimit-Reset", time.time() + 60)
                    )
                    wait = max(reset_time - int(time.time()), 1)
                    logger.warning(f"âš ï¸ Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§ØªØŒ Ø§Ù†ØªØ¸Ø§Ø± {wait} Ø«Ø§Ù†ÙŠØ©...")
                    time.sleep(min(wait, 60))
                    continue
                else:
                    logger.error(
                        f"âŒ Ø®Ø·Ø£ {response.status_code}: {response.text[:200]}"
                    )
                    
            except requests.exceptions.Timeout:
                logger.warning(f"â° Ø§Ù†ØªÙ‡Øª Ø§Ù„Ù…Ù‡Ù„Ø©ØŒ Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{retries}")
                time.sleep(2)
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£: {e}")
                time.sleep(2)
        
        return None
    
    def _refresh_sha(self, file_path: str):
        """ØªØ­Ø¯ÙŠØ« SHA Ù„Ù…Ù„Ù Ù…Ø¹ÙŠÙ†"""
        result = self._github_request("GET", f"contents/{file_path}?ref={self.branch}")
        if result and "sha" in result:
            self._cache_sha[file_path] = result["sha"]
    
    def _read_file(self, file_path: str) -> Optional[Any]:
        """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ù…Ù† GitHub"""
        result = self._github_request(
            "GET", f"contents/{file_path}?ref={self.branch}"
        )
        
        if result and "content" in result:
            self._cache_sha[file_path] = result["sha"]
            content = base64.b64decode(result["content"]).decode("utf-8")
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                return content
        
        return None
    
    def _write_file(self, file_path: str, data: Any, 
                     message: str = None) -> bool:
        """ÙƒØªØ§Ø¨Ø© Ù…Ù„Ù Ø¥Ù„Ù‰ GitHub"""
        if message is None:
            message = f"ğŸ“¦ ØªØ­Ø¯ÙŠØ« {file_path} - {datetime.now().isoformat()}"
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ JSON
        if isinstance(data, (dict, list)):
            content = json.dumps(data, ensure_ascii=False, indent=2)
        else:
            content = str(data)
        
        # ØªØ´ÙÙŠØ± Ø¨Ù€ base64
        encoded = base64.b64encode(content.encode("utf-8")).decode("utf-8")
        
        payload = {
            "message": message,
            "content": encoded,
            "branch": self.branch
        }
        
        # Ø¥Ø¶Ø§ÙØ© SHA Ø¥Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ (Ù„Ù„ØªØ­Ø¯ÙŠØ«)
        if file_path in self._cache_sha:
            payload["sha"] = self._cache_sha[file_path]
        
        result = self._github_request("PUT", f"contents/{file_path}", payload)
        
        if result and "content" in result:
            self._cache_sha[file_path] = result["content"]["sha"]
            logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ {file_path}")
            return True
        
        return False
    
    # ============================================
    # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    # ============================================
    
    def _load_all_data(self):
        """ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† GitHub Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡"""
        logger.info("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† GitHub...")
        
        data_files = [
            "data/database.json",
            "data/sessions.json", 
            "data/memory.json",
            "data/users.json"
        ]
        
        for file_path in data_files:
            data = self._read_file(file_path)
            if data is not None:
                self._cache[file_path] = data
                logger.info(f"  âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {file_path}")
            else:
                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ ÙØ§Ø±Øº
                self._cache[file_path] = {}
                self._write_file(file_path, {}, f"ğŸ†• Ø¥Ù†Ø´Ø§Ø¡ {file_path}")
                logger.info(f"  ğŸ†• ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {file_path}")
        
        logger.info(f"ğŸ“¥ ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self._cache)} Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª")
    
    def _start_auto_save(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ø­ÙØ¸ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        def auto_save():
            while True:
                time.sleep(self._auto_save_interval)
                self.save_all()
        
        thread = threading.Thread(target=auto_save, daemon=True)
        thread.start()
        logger.info(
            f"â° Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙƒÙ„ {self._auto_save_interval} Ø«Ø§Ù†ÙŠØ©"
        )
    
    def save_all(self):
        """Ø­ÙØ¸ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø©"""
        with self._lock:
            if not self._dirty:
                return
            
            dirty_copy = self._dirty.copy()
            self._dirty.clear()
        
        for file_path in dirty_copy:
            if file_path in self._cache:
                success = self._write_file(self._cache[file_path], file_path)
                if not success:
                    # Ø¥Ø¹Ø§Ø¯Ø© Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø­ÙØ¸
                    with self._lock:
                        self._dirty.add(file_path)
    
    def force_save_all(self):
        """Ø­ÙØ¸ Ø¥Ø¬Ø¨Ø§Ø±ÙŠ Ù„ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        with self._lock:
            for file_path, data in self._cache.items():
                self._write_file(file_path, data)
        logger.info("ğŸ’¾ ØªÙ… Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ Ù„ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    # ============================================
    # ÙˆØ§Ø¬Ù‡Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    # ============================================
    
    def get_db(self, collection: str = "default") -> dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª"""
        file_path = "data/database.json"
        with self._lock:
            if file_path not in self._cache:
                self._cache[file_path] = {}
            db = self._cache[file_path]
            if collection not in db:
                db[collection] = {}
            return db[collection]
    
    def set_value(self, collection: str, key: str, value: Any):
        """ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ…Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        file_path = "data/database.json"
        with self._lock:
            if file_path not in self._cache:
                self._cache[file_path] = {}
            if collection not in self._cache[file_path]:
                self._cache[file_path][collection] = {}
            self._cache[file_path][collection][key] = value
            self._dirty.add(file_path)
    
    def get_value(self, collection: str, key: str, 
                   default: Any = None) -> Any:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        file_path = "data/database.json"
        with self._lock:
            db = self._cache.get(file_path, {})
            return db.get(collection, {}).get(key, default)
    
    def delete_value(self, collection: str, key: str) -> bool:
        """Ø­Ø°Ù Ù‚ÙŠÙ…Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        file_path = "data/database.json"
        with self._lock:
            if file_path in self._cache:
                if collection in self._cache[file_path]:
                    if key in self._cache[file_path][collection]:
                        del self._cache[file_path][collection][key]
                        self._dirty.add(file_path)
                        return True
        return False
    
    def list_keys(self, collection: str) -> list:
        """Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        file_path = "data/database.json"
        with self._lock:
            db = self._cache.get(file_path, {})
            return list(db.get(collection, {}).keys())
    
    def search(self, collection: str, 
                query: Dict[str, Any]) -> List[dict]:
        """Ø¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª"""
        file_path = "data/database.json"
        results = []
        with self._lock:
            db = self._cache.get(file_path, {})
            items = db.get(collection, {})
            for key, value in items.items():
                if isinstance(value, dict):
                    match = all(
                        value.get(qk) == qv 
                        for qk, qv in query.items()
                    )
                    if match:
                        results.append({"_key": key, **value})
        return results
    
    # ============================================
    # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¬Ù„Ø³Ø§Øª (Sessions)
    # ============================================
    
    def save_session(self, session_name: str, session_data: Any):
        """Ø­ÙØ¸ Ø¬Ù„Ø³Ø©"""
        file_path = "data/sessions.json"
        with self._lock:
            if file_path not in self._cache:
                self._cache[file_path] = {}
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª bytesØŒ Ù†Ø­ÙˆÙ„Ù‡Ø§ Ù„Ù€ base64
            if isinstance(session_data, bytes):
                self._cache[file_path][session_name] = {
                    "type": "bytes",
                    "data": base64.b64encode(session_data).decode("utf-8"),
                    "saved_at": datetime.now().isoformat()
                }
            elif isinstance(session_data, str):
                self._cache[file_path][session_name] = {
                    "type": "string",
                    "data": session_data,
                    "saved_at": datetime.now().isoformat()
                }
            else:
                self._cache[file_path][session_name] = {
                    "type": "json",
                    "data": session_data,
                    "saved_at": datetime.now().isoformat()
                }
            
            self._dirty.add(file_path)
        
        # Ø­ÙØ¸ ÙÙˆØ±ÙŠ Ù„Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        self._write_file(file_path, self._cache[file_path])
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¬Ù„Ø³Ø©: {session_name}")
    
    def load_session(self, session_name: str) -> Optional[Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù„Ø³Ø©"""
        file_path = "data/sessions.json"
        with self._lock:
            sessions = self._cache.get(file_path, {})
            session = sessions.get(session_name)
            
            if session is None:
                return None
            
            if session["type"] == "bytes":
                return base64.b64decode(session["data"])
            elif session["type"] == "string":
                return session["data"]
            else:
                return session["data"]
    
    def delete_session(self, session_name: str) -> bool:
        """Ø­Ø°Ù Ø¬Ù„Ø³Ø©"""
        file_path = "data/sessions.json"
        with self._lock:
            if file_path in self._cache:
                if session_name in self._cache[file_path]:
                    del self._cache[file_path][session_name]
                    self._dirty.add(file_path)
                    return True
        return False
    
    def list_sessions(self) -> list:
        """Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ù„Ø³Ø§Øª"""
        file_path = "data/sessions.json"
        with self._lock:
            return list(self._cache.get(file_path, {}).keys())
    
    # ============================================
    # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Memory)
    # ============================================
    
    def remember(self, key: str, value: Any, 
                  category: str = "general"):
        """Ø­ÙØ¸ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        file_path = "data/memory.json"
        with self._lock:
            if file_path not in self._cache:
                self._cache[file_path] = {}
            if category not in self._cache[file_path]:
                self._cache[file_path][category] = {}
            
            self._cache[file_path][category][key] = {
                "value": value,
                "remembered_at": datetime.now().isoformat(),
                "access_count": 0
            }
            self._dirty.add(file_path)
    
    def recall(self, key: str, category: str = "general") -> Optional[Any]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø´ÙŠØ¡ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        file_path = "data/memory.json"
        with self._lock:
            memory = self._cache.get(file_path, {})
            cat = memory.get(category, {})
            item = cat.get(key)
            
            if item:
                item["access_count"] = item.get("access_count", 0) + 1
                item["last_accessed"] = datetime.now().isoformat()
                self._dirty.add(file_path)
                return item["value"]
        
        return None
    
    def forget(self, key: str, category: str = "general") -> bool:
        """Ù†Ø³ÙŠØ§Ù† Ø´ÙŠØ¡ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        file_path = "data/memory.json"
        with self._lock:
            memory = self._cache.get(file_path, {})
            if category in memory and key in memory[category]:
                del memory[category][key]
                self._dirty.add(file_path)
                return True
        return False
    
    def recall_all(self, category: str = "general") -> dict:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙƒÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ ÙØ¦Ø©"""
        file_path = "data/memory.json"
        with self._lock:
            memory = self._cache.get(file_path, {})
            cat = memory.get(category, {})
            return {k: v["value"] for k, v in cat.items()}
    
    # ============================================
    # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
    # ============================================
    
    def save_user(self, user_id: str, user_data: dict):
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³ØªØ®Ø¯Ù…"""
        file_path = "data/users.json"
        with self._lock:
            if file_path not in self._cache:
                self._cache[file_path] = {}
            
            if user_id in self._cache[file_path]:
                self._cache[file_path][user_id].update(user_data)
            else:
                self._cache[file_path][user_id] = user_data
            
            self._cache[file_path][user_id]["updated_at"] = (
                datetime.now().isoformat()
            )
            self._dirty.add(file_path)
    
    def get_user(self, user_id: str) -> Optional[dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³ØªØ®Ø¯Ù…"""
        file_path = "data/users.json"
        with self._lock:
            return self._cache.get(file_path, {}).get(user_id)
    
    def get_all_users(self) -> dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
        file_path = "data/users.json"
        with self._lock:
            return self._cache.get(file_path, {}).copy()
    
    def delete_user(self, user_id: str) -> bool:
        """Ø­Ø°Ù Ù…Ø³ØªØ®Ø¯Ù…"""
        file_path = "data/users.json"
        with self._lock:
            if file_path in self._cache:
                if user_id in self._cache[file_path]:
                    del self._cache[file_path][user_id]
                    self._dirty.add(file_path)
                    return True
        return False
    
    # ============================================
    # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (ØªÙ‚Ø³ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ)
    # ============================================
    
    def save_large_data(self, name: str, data: Any) -> bool:
        """
        Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© - ÙŠÙ‚Ø³Ù…Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ²Øª 50MB
        GitHub ÙŠØ³Ù…Ø­ Ø¨Ù…Ù„ÙØ§Øª Ø­ØªÙ‰ 100MB
        """
        content = json.dumps(data, ensure_ascii=False)
        size_mb = len(content.encode("utf-8")) / (1024 * 1024)
        
        if size_mb < 50:
            # Ù…Ù„Ù ÙˆØ§Ø­Ø¯
            return self._write_file(f"data/large/{name}.json", data)
        else:
            # ØªÙ‚Ø³ÙŠÙ…
            chunk_size = 40 * 1024 * 1024  # 40MB per chunk
            chunks = []
            content_bytes = content.encode("utf-8")
            
            for i in range(0, len(content_bytes), chunk_size):
                chunk = content_bytes[i:i + chunk_size]
                chunks.append(chunk)
            
            # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
            meta = {
                "name": name,
                "total_chunks": len(chunks),
                "total_size": len(content_bytes),
                "created_at": datetime.now().isoformat()
            }
            self._write_file(f"data/large/{name}_meta.json", meta)
            
            # Ø­ÙØ¸ ÙƒÙ„ Ø¬Ø²Ø¡
            for i, chunk in enumerate(chunks):
                chunk_b64 = base64.b64encode(chunk).decode("utf-8")
                self._write_file(
                    f"data/large/{name}_chunk_{i}.json",
                    {"chunk": i, "data": chunk_b64}
                )
            
            logger.info(
                f"âœ… ØªÙ… Ø­ÙØ¸ {name} ({size_mb:.1f}MB) "
                f"ÙÙŠ {len(chunks)} Ø£Ø¬Ø²Ø§Ø¡"
            )
            return True
    
    def load_large_data(self, name: str) -> Optional[Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©"""
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø£ÙˆÙ„Ø§Ù‹
        data = self._read_file(f"data/large/{name}.json")
        if data is not None:
            return data
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ù„Ù Ù…Ù‚Ø³Ù…
        meta = self._read_file(f"data/large/{name}_meta.json")
        if meta is None:
            return None
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
        all_bytes = b""
        for i in range(meta["total_chunks"]):
            chunk_data = self._read_file(
                f"data/large/{name}_chunk_{i}.json"
            )
            if chunk_data:
                all_bytes += base64.b64decode(chunk_data["data"])
        
        return json.loads(all_bytes.decode("utf-8"))
    
    # ============================================
    # Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ
    # ============================================
    
    def create_backup(self) -> bool:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for file_path, data in self._cache.items():
            backup_path = file_path.replace(
                "data/", f"data/backup/{timestamp}/"
            )
            self._write_file(
                backup_path, data,
                f"ğŸ”’ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© {timestamp}"
            )
        
        logger.info(f"ğŸ”’ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {timestamp}")
        return True
    
    # ============================================
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    # ============================================
    
    def get_stats(self) -> dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ†"""
        stats = {
            "total_files": len(self._cache),
            "dirty_files": len(self._dirty),
            "collections": {},
            "sessions_count": 0,
            "memory_categories": 0,
            "users_count": 0
        }
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        db = self._cache.get("data/database.json", {})
        for col_name, col_data in db.items():
            stats["collections"][col_name] = len(col_data)
        
        # Ø§Ù„Ø¬Ù„Ø³Ø§Øª
        sessions = self._cache.get("data/sessions.json", {})
        stats["sessions_count"] = len(sessions)
        
        # Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory = self._cache.get("data/memory.json", {})
        stats["memory_categories"] = len(memory)
        stats["total_memories"] = sum(
            len(v) for v in memory.values()
        )
        
        # Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        users = self._cache.get("data/users.json", {})
        stats["users_count"] = len(users)
        
        return stats
    
    def __del__(self):
        """Ø­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¹Ù†Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚"""
        try:
            self.force_save_all()
        except Exception:
            pass


# ============================================
# Singleton - Ù†Ø³Ø®Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ†
# ============================================

_storage_instance = None


def get_storage() -> GitHubStorage:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† (Singleton)"""
    global _storage_instance
    if _storage_instance is None:
        _storage_instance = GitHubStorage()
    return _storage_instance
